{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST with Batch Normalization.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqfyeSGFidCvGUKpKL+yEe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeush-agarwal/week-based-learning/blob/master/Deep-Learning/Batch-Normalization/MNIST_with_Batch_Normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUQsIkFa4KuB",
        "colab_type": "text"
      },
      "source": [
        "# Improvement on custom Neural network for MNIST Dataset\n",
        "\n",
        "Outline:\n",
        "+ Load MNIST dataset (Train, test)\n",
        "+ Build a custom Fully connected Neural network model\n",
        "+ Build a custom Convolutional Neural Network model\n",
        "+ Train and eval different models\n",
        "+ Apply **Batch Normalization** on both the above models\n",
        "+ Depict the difference on both the models (before and after Batch Norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb6dPrnW492U",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVp2RQVq4JHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('dark_background')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li8mqWxq5bXm",
        "colab_type": "text"
      },
      "source": [
        "## Load MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-Z3-UBY32P3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset = datasets.MNIST('./Data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "testset = datasets.MNIST('./Data', train=False, transform=transforms.ToTensor(), download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGGLauUl6TLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvA_ci2v6UmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0rbl1zW6x0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = trainset.classes\n",
        "print (classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toXr39Qd6ntS",
        "colab_type": "text"
      },
      "source": [
        "## Visualize sample images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoewb3Ec6mCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(images, labels, n_samples = 4):\n",
        "  images = images[:n_samples]\n",
        "  labels = labels[:n_samples]\n",
        "\n",
        "  images = torchvision.utils.make_grid(images)\n",
        "  title = f'Labels:{[label.item() for label in labels]}'\n",
        "\n",
        "  plt.figure(figsize=(n_samples * 4, 4))\n",
        "  plt.imshow(np.transpose(images, (1,2,0)))\n",
        "  plt.title(title)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Iakd6oG67qE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, labels = next(iter(trainloader))\n",
        "\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "imshow(images, labels, n_samples=batch_size if batch_size <= 8 else 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WagaB5qIDlrv",
        "colab_type": "text"
      },
      "source": [
        "## Get computing device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3igGhzNDob8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGWRy5ct__lb",
        "colab_type": "text"
      },
      "source": [
        "## Build custom Fully connected NeuralNetwork for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDII2TGv8AQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_NN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MNIST_NN, self).__init__()\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(784, 48),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(48, 24),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(24, 10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 784)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONJCGy4hHTWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_NN_BatchNorm(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MNIST_NN_BatchNorm, self).__init__()\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(784, 48),\n",
        "        nn.BatchNorm1d(48), # 1d because we have 1 channel only, for 3 channels we use 2d. 48 argument = no. of output features from previous layer\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(48, 24),\n",
        "        nn.BatchNorm1d(24),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(24, 10)\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QL49e-dIKLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIxycYD8A46d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model without Batch Normalization\n",
        "fc_model = MNIST_NN().to(device)\n",
        "fc_optimizer = optim.SGD(fc_model.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojHzDf_hIQep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fc_batchnorm_model = MNIST_NN_BatchNorm().to(device)\n",
        "fc_batchnorm_optimizer = optim.SGD(fc_batchnorm_model.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cZsYnwQdF_Di",
        "colab": {}
      },
      "source": [
        "batch_size = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bC-w3RWzF_Do",
        "colab": {}
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxIty_avB6dp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train both models simulataneously on same data\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "loss_arr = []\n",
        "loss_bn_arr = []\n",
        "for epoch in range(epochs):\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    images, labels = data\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Train FC without BatchNorm model\n",
        "    fc_optimizer.zero_grad()\n",
        "    outputs = fc_model(images)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "    fc_optimizer.step()\n",
        "\n",
        "    # Train FC with BatchNorm model\n",
        "    fc_batchnorm_optimizer.zero_grad()\n",
        "    outputs1 = fc_batchnorm_model(images)\n",
        "    loss1 = loss_fn(outputs1, labels)\n",
        "    loss1.backward()\n",
        "    fc_batchnorm_optimizer.step()\n",
        "\n",
        "    loss_arr.append(loss.item())\n",
        "    loss_bn_arr.append(loss1.item())\n",
        "\n",
        "    if i%10 == 0:\n",
        "      # Display distribution plots of learning after BatchNorm layer\n",
        "      inputs = images.view(images.size(0), -1)\n",
        "\n",
        "      fc_model.eval()\n",
        "      fc_batchnorm_model.eval()\n",
        "\n",
        "      a = fc_model.classifier[0](inputs)    # Linear(784, 48)\n",
        "      a = fc_model.classifier[1](a)         # Relu()\n",
        "      a = fc_model.classifier[2](a)         # Linear(48, 24)\n",
        "      a = a.detach().numpy().ravel()        # Flatten \n",
        "      sns.distplot(a, kde=True, color='r', label='W/o Batch Norm')\n",
        "\n",
        "      b = fc_batchnorm_model.classifier[0](inputs)  # Linear(784, 48)\n",
        "      b = fc_batchnorm_model.classifier[1](b)       # BatchNorm1d(48)\n",
        "      b = fc_batchnorm_model.classifier[2](b)       # Relu()\n",
        "      b = fc_batchnorm_model.classifier[3](b)       # Linear(48, 24)\n",
        "      b = fc_batchnorm_model.classifier[4](b)       # BatchNorm1d(24)\n",
        "      b = b.detach().numpy().ravel()                # Flatten \n",
        "      sns.distplot(b, kde=True, color='g', label='W/ Batch Norm')\n",
        "\n",
        "      plt.title(f'iteration:{i}, loss:{loss.item():.2f}, loss_bn:{loss1.item():.2f}')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "      fc_model.train()\n",
        "      fc_batchnorm_model.train()\n",
        "\n",
        "plt.plot(loss_arr, 'r', label='W/o BatchNorm')\n",
        "plt.plot(loss_bn_arr, 'g', label='W/ BatchNorm')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpETB2SDKXZJ",
        "colab_type": "text"
      },
      "source": [
        "It depicts that Batch normalization technique helps it to reduce loss compared to traditional technique. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mACRdCX6FXqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}